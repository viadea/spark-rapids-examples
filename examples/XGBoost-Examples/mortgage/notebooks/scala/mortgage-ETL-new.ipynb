{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82e9fb4",
   "metadata": {},
   "source": [
    "# Introduction to Mortgage ETL Job\n",
    "This is the mortgage ETL job to generate the input datasets for the mortgage Xgboost job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8c3fa",
   "metadata": {},
   "source": [
    "## Prerequirement\n",
    "### 1. Download data\n",
    "Dataset is downloaded from [Fannie Maeâ€™s Single-Family Loan Performance Data](https://capitalmarkets.fanniemae.com/credit-risk-transfer/single-family-credit-risk-transfer/fannie-mae-single-family-loan-performance-data) with all rights reserved by Fannie Mae.\n",
    "\n",
    "### 2. Download needed jars\n",
    "* [cudf-22.04.0-cuda11.jar](https://repo1.maven.org/maven2/ai/rapids/cudf/22.04.0/)\n",
    "* [rapids-4-spark_2.12-22.04.0.jar](https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/22.04.0/rapids-4-spark_2.12-22.04.0.jar)\n",
    "\n",
    "### 3. Start Spark Standalone\n",
    "Before Running the script, please setup Spark standalone mode\n",
    "\n",
    "### 4. Add ENV\n",
    "```\n",
    "$ export SPARK_JARS=cudf-22.04.0-cuda11.jar,rapids-4-spark_2.12-22.04.0.jar\n",
    "\n",
    "```\n",
    "\n",
    "### 5.Start Jupyter Notebook with spylon-kernal or toree\n",
    "\n",
    "```\n",
    "$ jupyter notebook --allow-root --notebook-dir=${your-dir} --config=${your-configs}\n",
    "```\n",
    "\n",
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ecc912c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://hazhu-dt:4040\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = spark://hazhu-dt:7077, app id = app-20220526191430-0014)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.fs.Path\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.{Column, DataFrame, SparkSession}\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.hadoop.fs.Path\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{Column, DataFrame, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "exotic-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.rapids.sql.csv.read.double.enabled\",true)\n",
    "spark.conf.set(\"spark.rapids.sql.enabled\",true)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58fcd6d",
   "metadata": {},
   "source": [
    "## Script Settings\n",
    "\n",
    "### 1. File Path Settings\n",
    "* Define input file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "674771a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataRoot: String = /home/hazhu/data/mortgage_raw\n",
       "rawPath: String = /home/hazhu/data/mortgage_raw/input/\n",
       "outPath: String = /home/hazhu/data/mortgage_raw/output/\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataRoot = sys.env.getOrElse(\"DATA_ROOT\", \"/home/hazhu/data/mortgage_raw\")\n",
    "val rawPath = dataRoot + \"/input/\"\n",
    "val outPath = dataRoot + \"/output/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a2c7b",
   "metadata": {},
   "source": [
    "## Function and Object Define\n",
    "### 1. Define the constants\n",
    "\n",
    "* Define input/output file schema (Performance and Acquisition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e557beb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "performanceSchema: org.apache.spark.sql.types.StructType = StructType(StructField(loan_id,LongType,true), StructField(monthly_reporting_period,StringType,true), StructField(servicer,StringType,true), StructField(interest_rate,DoubleType,true), StructField(current_actual_upb,DoubleType,true), StructField(loan_age,DoubleType,true), StructField(remaining_months_to_legal_maturity,DoubleType,true), StructField(adj_remaining_months_to_maturity,DoubleType,true), StructField(maturity_date,StringType,true), StructField(msa,DoubleType,true), StructField(current_loan_delinquency_status,IntegerType,true), StructField(mod_flag,StringType,true), StructField(zero_balance_code,StringType,true), StructField(zero_balance_effective_date,StringType,true), StructField(last_paid_installment_date,StringType,t...\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// File schema\n",
    "val performanceSchema = StructType(Array(\n",
    "      StructField(\"loan_id\", LongType),\n",
    "      StructField(\"monthly_reporting_period\", StringType),\n",
    "      StructField(\"servicer\", StringType),\n",
    "      StructField(\"interest_rate\", DoubleType),\n",
    "      StructField(\"current_actual_upb\", DoubleType),\n",
    "      StructField(\"loan_age\", DoubleType),\n",
    "      StructField(\"remaining_months_to_legal_maturity\", DoubleType),\n",
    "      StructField(\"adj_remaining_months_to_maturity\", DoubleType),\n",
    "      StructField(\"maturity_date\", StringType),\n",
    "      StructField(\"msa\", DoubleType),\n",
    "      StructField(\"current_loan_delinquency_status\", IntegerType),\n",
    "      StructField(\"mod_flag\", StringType),\n",
    "      StructField(\"zero_balance_code\", StringType),\n",
    "      StructField(\"zero_balance_effective_date\", StringType),\n",
    "      StructField(\"last_paid_installment_date\", StringType),\n",
    "      StructField(\"foreclosed_after\", StringType),\n",
    "      StructField(\"disposition_date\", StringType),\n",
    "      StructField(\"foreclosure_costs\", DoubleType),\n",
    "      StructField(\"prop_preservation_and_repair_costs\", DoubleType),\n",
    "      StructField(\"asset_recovery_costs\", DoubleType),\n",
    "      StructField(\"misc_holding_expenses\", DoubleType),\n",
    "      StructField(\"holding_taxes\", DoubleType),\n",
    "      StructField(\"net_sale_proceeds\", DoubleType),\n",
    "      StructField(\"credit_enhancement_proceeds\", DoubleType),\n",
    "      StructField(\"repurchase_make_whole_proceeds\", StringType),\n",
    "      StructField(\"other_foreclosure_proceeds\", DoubleType),\n",
    "      StructField(\"non_interest_bearing_upb\", DoubleType),\n",
    "      StructField(\"principal_forgiveness_upb\", StringType),\n",
    "      StructField(\"repurchase_make_whole_proceeds_flag\", StringType),\n",
    "      StructField(\"foreclosure_principal_write_off_amount\", StringType),\n",
    "      StructField(\"servicing_activity_indicator\", StringType))\n",
    "    )\n",
    "\n",
    "val acquisitionSchema = StructType(Array(\n",
    "      StructField(\"loan_id\", LongType),\n",
    "      StructField(\"orig_channel\", StringType),\n",
    "      StructField(\"seller_name\", StringType),\n",
    "      StructField(\"orig_interest_rate\", DoubleType),\n",
    "      StructField(\"orig_upb\", IntegerType),\n",
    "      StructField(\"orig_loan_term\", IntegerType),\n",
    "      StructField(\"orig_date\", StringType),\n",
    "      StructField(\"first_pay_date\", StringType),\n",
    "      StructField(\"orig_ltv\", DoubleType),\n",
    "      StructField(\"orig_cltv\", DoubleType),\n",
    "      StructField(\"num_borrowers\", DoubleType),\n",
    "      StructField(\"dti\", DoubleType),\n",
    "      StructField(\"borrower_credit_score\", DoubleType),\n",
    "      StructField(\"first_home_buyer\", StringType),\n",
    "      StructField(\"loan_purpose\", StringType),\n",
    "      StructField(\"property_type\", StringType),\n",
    "      StructField(\"num_units\", IntegerType),\n",
    "      StructField(\"occupancy_status\", StringType),\n",
    "      StructField(\"property_state\", StringType),\n",
    "      StructField(\"zip\", IntegerType),\n",
    "      StructField(\"mortgage_insurance_percent\", DoubleType),\n",
    "      StructField(\"product_type\", StringType),\n",
    "      StructField(\"coborrow_credit_score\", DoubleType),\n",
    "      StructField(\"mortgage_insurance_type\", DoubleType),\n",
    "      StructField(\"relocation_mortgage_indicator\", StringType))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af48b6",
   "metadata": {},
   "source": [
    "* Define seller name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69f193d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object NameMapping\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object NameMapping {\n",
    "  /**\n",
    "    * Returns a dataframe with two columns named based off of the column names passed in.\n",
    "    * The fromColName has the original name we want to clean up, the toColName\n",
    "    * will have the name we want to go to, the unambiguous name.\n",
    "    */\n",
    "  def apply(spark: SparkSession, fromColName: String, toColName: String): DataFrame = {\n",
    "    import spark.sqlContext.implicits._\n",
    "    broadcast(Seq(\n",
    "      (\"WITMER FUNDING, LLC\", \"Witmer\"),\n",
    "      (\"WELLS FARGO CREDIT RISK TRANSFER SECURITIES TRUST 2015\", \"Wells Fargo\"),\n",
    "      (\"WELLS FARGO BANK,  NA\" , \"Wells Fargo\"),\n",
    "      (\"WELLS FARGO BANK, N.A.\" , \"Wells Fargo\"),\n",
    "      (\"WELLS FARGO BANK, NA\" , \"Wells Fargo\"),\n",
    "      (\"USAA FEDERAL SAVINGS BANK\" , \"USAA\"),\n",
    "      (\"UNITED SHORE FINANCIAL SERVICES, LLC D\\\\/B\\\\/A UNITED WHOLESALE MORTGAGE\" , \"United Seq(e\"),\n",
    "      (\"U.S. BANK N.A.\" , \"US Bank\"),\n",
    "      (\"SUNTRUST MORTGAGE INC.\" , \"Suntrust\"),\n",
    "      (\"STONEGATE MORTGAGE CORPORATION\" , \"Stonegate Mortgage\"),\n",
    "      (\"STEARNS LENDING, LLC\" , \"Stearns Lending\"),\n",
    "      (\"STEARNS LENDING, INC.\" , \"Stearns Lending\"),\n",
    "      (\"SIERRA PACIFIC MORTGAGE COMPANY, INC.\" , \"Sierra Pacific Mortgage\"),\n",
    "      (\"REGIONS BANK\" , \"Regions\"),\n",
    "      (\"RBC MORTGAGE COMPANY\" , \"RBC\"),\n",
    "      (\"QUICKEN LOANS INC.\" , \"Quicken Loans\"),\n",
    "      (\"PULTE MORTGAGE, L.L.C.\" , \"Pulte Mortgage\"),\n",
    "      (\"PROVIDENT FUNDING ASSOCIATES, L.P.\" , \"Provident Funding\"),\n",
    "      (\"PROSPECT MORTGAGE, LLC\" , \"Prospect Mortgage\"),\n",
    "      (\"PRINCIPAL RESIDENTIAL MORTGAGE CAPITAL RESOURCES, LLC\" , \"Principal Residential\"),\n",
    "      (\"PNC BANK, N.A.\" , \"PNC\"),\n",
    "      (\"PMT CREDIT RISK TRANSFER TRUST 2015-2\" , \"PennyMac\"),\n",
    "      (\"PHH MORTGAGE CORPORATION\" , \"PHH Mortgage\"),\n",
    "      (\"PENNYMAC CORP.\" , \"PennyMac\"),\n",
    "      (\"PACIFIC UNION FINANCIAL, LLC\" , \"Other\"),\n",
    "      (\"OTHER\" , \"Other\"),\n",
    "      (\"NYCB MORTGAGE COMPANY, LLC\" , \"NYCB\"),\n",
    "      (\"NEW YORK COMMUNITY BANK\" , \"NYCB\"),\n",
    "      (\"NETBANK FUNDING SERVICES\" , \"Netbank\"),\n",
    "      (\"NATIONSTAR MORTGAGE, LLC\" , \"Nationstar Mortgage\"),\n",
    "      (\"METLIFE BANK, NA\" , \"Metlife\"),\n",
    "      (\"LOANDEPOT.COM, LLC\" , \"LoanDepot.com\"),\n",
    "      (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2015-1\" , \"JP Morgan Chase\"),\n",
    "      (\"J.P. MORGAN MADISON AVENUE SECURITIES TRUST, SERIES 2014-1\" , \"JP Morgan Chase\"),\n",
    "      (\"JPMORGAN CHASE BANK, NATIONAL ASSOCIATION\" , \"JP Morgan Chase\"),\n",
    "      (\"JPMORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n",
    "      (\"JP MORGAN CHASE BANK, NA\" , \"JP Morgan Chase\"),\n",
    "      (\"IRWIN MORTGAGE, CORPORATION\" , \"Irwin Mortgage\"),\n",
    "      (\"IMPAC MORTGAGE CORP.\" , \"Impac Mortgage\"),\n",
    "      (\"HSBC BANK USA, NATIONAL ASSOCIATION\" , \"HSBC\"),\n",
    "      (\"HOMEWARD RESIDENTIAL, INC.\" , \"Homeward Mortgage\"),\n",
    "      (\"HOMESTREET BANK\" , \"Other\"),\n",
    "      (\"HOMEBRIDGE FINANCIAL SERVICES, INC.\" , \"HomeBridge\"),\n",
    "      (\"HARWOOD STREET FUNDING I, LLC\" , \"Harwood Mortgage\"),\n",
    "      (\"GUILD MORTGAGE COMPANY\" , \"Guild Mortgage\"),\n",
    "      (\"GMAC MORTGAGE, LLC (USAA FEDERAL SAVINGS BANK)\" , \"GMAC\"),\n",
    "      (\"GMAC MORTGAGE, LLC\" , \"GMAC\"),\n",
    "      (\"GMAC (USAA)\" , \"GMAC\"),\n",
    "      (\"FREMONT BANK\" , \"Fremont Bank\"),\n",
    "      (\"FREEDOM MORTGAGE CORP.\" , \"Freedom Mortgage\"),\n",
    "      (\"FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"Franklin America\"),\n",
    "      (\"FLEET NATIONAL BANK\" , \"Fleet National\"),\n",
    "      (\"FLAGSTAR CAPITAL MARKETS CORPORATION\" , \"Flagstar Bank\"),\n",
    "      (\"FLAGSTAR BANK, FSB\" , \"Flagstar Bank\"),\n",
    "      (\"FIRST TENNESSEE BANK NATIONAL ASSOCIATION\" , \"Other\"),\n",
    "      (\"FIFTH THIRD BANK\" , \"Fifth Third Bank\"),\n",
    "      (\"FEDERAL HOME LOAN BANK OF CHICAGO\" , \"Fedral Home of Chicago\"),\n",
    "      (\"FDIC, RECEIVER, INDYMAC FEDERAL BANK FSB\" , \"FDIC\"),\n",
    "      (\"DOWNEY SAVINGS AND LOAN ASSOCIATION, F.A.\" , \"Downey Mortgage\"),\n",
    "      (\"DITECH FINANCIAL LLC\" , \"Ditech\"),\n",
    "      (\"CITIMORTGAGE, INC.\" , \"Citi\"),\n",
    "      (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERFIRST MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n",
    "      (\"CHICAGO MORTGAGE SOLUTIONS DBA INTERBANK MORTGAGE COMPANY\" , \"Chicago Mortgage\"),\n",
    "      (\"CHASE HOME FINANCE, LLC\" , \"JP Morgan Chase\"),\n",
    "      (\"CHASE HOME FINANCE FRANKLIN AMERICAN MORTGAGE COMPANY\" , \"JP Morgan Chase\"),\n",
    "      (\"CHASE HOME FINANCE (CIE 1)\" , \"JP Morgan Chase\"),\n",
    "      (\"CHASE HOME FINANCE\" , \"JP Morgan Chase\"),\n",
    "      (\"CASHCALL, INC.\" , \"CashCall\"),\n",
    "      (\"CAPITAL ONE, NATIONAL ASSOCIATION\" , \"Capital One\"),\n",
    "      (\"CALIBER HOME LOANS, INC.\" , \"Caliber Funding\"),\n",
    "      (\"BISHOPS GATE RESIDENTIAL MORTGAGE TRUST\" , \"Bishops Gate Mortgage\"),\n",
    "      (\"BANK OF AMERICA, N.A.\" , \"Bank of America\"),\n",
    "      (\"AMTRUST BANK\" , \"AmTrust\"),\n",
    "      (\"AMERISAVE MORTGAGE CORPORATION\" , \"Amerisave\"),\n",
    "      (\"AMERIHOME MORTGAGE COMPANY, LLC\" , \"AmeriHome Mortgage\"),\n",
    "      (\"ALLY BANK\" , \"Ally Bank\"),\n",
    "      (\"ACADEMY MORTGAGE CORPORATION\" , \"Academy Mortgage\"),\n",
    "      (\"NO CASH-OUT REFINANCE\" , \"OTHER REFINANCE\"),\n",
    "      (\"REFINANCE - NOT SPECIFIED\" , \"OTHER REFINANCE\"),\n",
    "      (\"Other REFINANCE\" , \"OTHER REFINANCE\")\n",
    "    ).toDF(fromColName, toColName))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "therapeutic-sustainability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawSchema: org.apache.spark.sql.types.StructType = StructType(StructField(reference_pool_id,StringType,true), StructField(loan_id,LongType,true), StructField(monthly_reporting_period,StringType,true), StructField(orig_channel,StringType,true), StructField(seller_name,StringType,true), StructField(servicer,StringType,true), StructField(master_servicer,StringType,true), StructField(orig_interest_rate,DoubleType,true), StructField(interest_rate,DoubleType,true), StructField(orig_upb,IntegerType,true), StructField(upb_at_issuance,StringType,true), StructField(current_actual_upb,DoubleType,true), StructField(orig_loan_term,IntegerType,true), StructField(orig_date,StringType,true), StructField(first_pay_date,StringType,true), StructField(loan_age,DoubleType,true), StructField(remaining_months...\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawSchema = StructType(Array(\n",
    "      StructField(\"reference_pool_id\", StringType),\n",
    "      StructField(\"loan_id\", LongType),\n",
    "      StructField(\"monthly_reporting_period\", StringType),\n",
    "      StructField(\"orig_channel\", StringType),\n",
    "      StructField(\"seller_name\", StringType),\n",
    "      StructField(\"servicer\", StringType),\n",
    "      StructField(\"master_servicer\", StringType),\n",
    "      StructField(\"orig_interest_rate\", DoubleType),\n",
    "      StructField(\"interest_rate\", DoubleType),\n",
    "      StructField(\"orig_upb\", IntegerType),\n",
    "      StructField(\"upb_at_issuance\", StringType),\n",
    "      StructField(\"current_actual_upb\", DoubleType),\n",
    "      StructField(\"orig_loan_term\", IntegerType),\n",
    "      StructField(\"orig_date\", StringType),\n",
    "      StructField(\"first_pay_date\", StringType),    \n",
    "      StructField(\"loan_age\", DoubleType),\n",
    "      StructField(\"remaining_months_to_legal_maturity\", DoubleType),\n",
    "      StructField(\"adj_remaining_months_to_maturity\", DoubleType),\n",
    "      StructField(\"maturity_date\", StringType),\n",
    "      StructField(\"orig_ltv\", DoubleType),\n",
    "      StructField(\"orig_cltv\", DoubleType),\n",
    "      StructField(\"num_borrowers\", DoubleType),\n",
    "      StructField(\"dti\", DoubleType),\n",
    "      StructField(\"borrower_credit_score\", DoubleType),\n",
    "      StructField(\"coborrow_credit_score\", DoubleType),\n",
    "      StructField(\"first_home_buyer\", StringType),\n",
    "      StructField(\"loan_purpose\", StringType),\n",
    "      StructField(\"property_type\", StringType),\n",
    "      StructField(\"num_units\", IntegerType),\n",
    "      StructField(\"occupancy_status\", StringType),\n",
    "      StructField(\"property_state\", StringType),\n",
    "      StructField(\"msa\", DoubleType),\n",
    "      StructField(\"zip\", IntegerType),\n",
    "      StructField(\"mortgage_insurance_percent\", DoubleType),\n",
    "      StructField(\"product_type\", StringType),\n",
    "      StructField(\"prepayment_penalty_indicator\", StringType),\n",
    "      StructField(\"interest_only_loan_indicator\", StringType),\n",
    "      StructField(\"interest_only_first_principal_and_interest_payment_date\", StringType),\n",
    "      StructField(\"months_to_amortization\", StringType),\n",
    "      StructField(\"current_loan_delinquency_status\", IntegerType),\n",
    "      StructField(\"loan_payment_history\", StringType),\n",
    "      StructField(\"mod_flag\", StringType),\n",
    "      StructField(\"mortgage_insurance_cancellation_indicator\", StringType),\n",
    "      StructField(\"zero_balance_code\", StringType),\n",
    "      StructField(\"zero_balance_effective_date\", StringType),\n",
    "      StructField(\"upb_at_the_time_of_removal\", StringType),\n",
    "      StructField(\"repurchase_date\", StringType),\n",
    "      StructField(\"scheduled_principal_current\", StringType),\n",
    "      StructField(\"total_principal_current\", StringType),\n",
    "      StructField(\"unscheduled_principal_current\", StringType),\n",
    "      StructField(\"last_paid_installment_date\", StringType),\n",
    "      StructField(\"foreclosed_after\", StringType),\n",
    "      StructField(\"disposition_date\", StringType),\n",
    "      StructField(\"foreclosure_costs\", DoubleType),\n",
    "      StructField(\"prop_preservation_and_repair_costs\", DoubleType),\n",
    "      StructField(\"asset_recovery_costs\", DoubleType),\n",
    "      StructField(\"misc_holding_expenses\", DoubleType),\n",
    "      StructField(\"holding_taxes\", DoubleType),\n",
    "      StructField(\"net_sale_proceeds\", DoubleType),\n",
    "      StructField(\"credit_enhancement_proceeds\", DoubleType),\n",
    "      StructField(\"repurchase_make_whole_proceeds\", StringType),\n",
    "      StructField(\"other_foreclosure_proceeds\", DoubleType),\n",
    "      StructField(\"non_interest_bearing_upb\", DoubleType),\n",
    "      StructField(\"principal_forgiveness_upb\", StringType),\n",
    "      StructField(\"original_list_start_date\", StringType),\n",
    "      StructField(\"original_list_price\", StringType),\n",
    "      StructField(\"current_list_start_date\", StringType),\n",
    "      StructField(\"current_list_price\", StringType),\n",
    "      StructField(\"borrower_credit_score_at_issuance\", StringType),\n",
    "      StructField(\"co-borrower_credit_score_at_issuance\", StringType),\n",
    "      StructField(\"borrower_credit_score_current\", StringType),\n",
    "      StructField(\"co-Borrower_credit_score_current\", StringType),\n",
    "      StructField(\"mortgage_insurance_type\", DoubleType),\n",
    "      StructField(\"servicing_activity_indicator\", StringType),\n",
    "      StructField(\"current_period_modification_loss_amount\", StringType),\n",
    "      StructField(\"cumulative_modification_loss_amount\", StringType),\n",
    "      StructField(\"current_period_credit_event_net_gain_or_loss\", StringType),\n",
    "      StructField(\"cumulative_credit_event_net_gain_or_loss\", StringType),\n",
    "      StructField(\"homeready_program_indicator\", StringType),\n",
    "      StructField(\"foreclosure_principal_write_off_amount\", StringType),\n",
    "      StructField(\"relocation_mortgage_indicator\", StringType),\n",
    "      StructField(\"zero_balance_code_change_date\", StringType),\n",
    "      StructField(\"loan_holdback_indicator\", StringType),\n",
    "      StructField(\"loan_holdback_effective_date\", StringType),\n",
    "      StructField(\"delinquent_accrued_interest\", StringType),\n",
    "      StructField(\"property_valuation_method\", StringType),\n",
    "      StructField(\"high_balance_loan_indicator\", StringType),\n",
    "      StructField(\"arm_initial_fixed-rate_period_lt_5_yr_indicator\", StringType),\n",
    "      StructField(\"arm_product_type\", StringType),\n",
    "      StructField(\"initial_fixed-rate_period\", StringType),\n",
    "      StructField(\"interest_rate_adjustment_frequency\", StringType),\n",
    "      StructField(\"next_interest_rate_adjustment_date\", StringType),\n",
    "      StructField(\"next_payment_change_date\", StringType),\n",
    "      StructField(\"index\", StringType),\n",
    "      StructField(\"arm_cap_structure\", StringType),\n",
    "      StructField(\"initial_interest_rate_cap_up_percent\", StringType),\n",
    "      StructField(\"periodic_interest_rate_cap_up_percent\", StringType),\n",
    "      StructField(\"lifetime_interest_rate_cap_up_percent\", StringType),\n",
    "      StructField(\"mortgage_margin\", StringType),\n",
    "      StructField(\"arm_balloon_indicator\", StringType),\n",
    "      StructField(\"arm_plan_number\", StringType),\n",
    "      StructField(\"borrower_assistance_plan\", StringType),\n",
    "      StructField(\"hltv_refinance_option_indicator\", StringType),\n",
    "      StructField(\"deal_name\", StringType),\n",
    "      StructField(\"repurchase_make_whole_proceeds_flag\", StringType),\n",
    "      StructField(\"alternative_delinquency_resolution\", StringType),\n",
    "      StructField(\"alternative_delinquency_resolution_count\", StringType),\n",
    "      StructField(\"total_deferral_amount\", StringType)\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42098a5a",
   "metadata": {},
   "source": [
    "### 2. Define ETL Process\n",
    "\n",
    "Define the function to do the ETL process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead44543",
   "metadata": {},
   "source": [
    "* Define category (string) column and numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9936e221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelColName: String = delinquency_12\n",
       "categaryCols: List[(String, org.apache.spark.sql.types.FloatType.type)] = List((orig_channel,FloatType), (first_home_buyer,FloatType), (loan_purpose,FloatType), (property_type,FloatType), (occupancy_status,FloatType), (property_state,FloatType), (product_type,FloatType), (relocation_mortgage_indicator,FloatType), (seller_name,FloatType), (mod_flag,FloatType))\n",
       "numericCols: List[(String, org.apache.spark.sql.types.NumericType with Product with Serializable)] = List((orig_interest_rate,FloatType), (orig_upb,IntegerType), (orig_loan_term,IntegerType), (orig_ltv,FloatType), (orig_cltv,FloatType), (num_borrowers,FloatType), (dti,FloatType), (borrower_credit_score,FloatType), (num_units,IntegerType), (zip,IntegerType), (mortgage_insurance_percent,FloatType...\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelColName = \"delinquency_12\"\n",
    "val categaryCols = List(\n",
    "    (\"orig_channel\", FloatType),\n",
    "    (\"first_home_buyer\", FloatType),\n",
    "    (\"loan_purpose\", FloatType),\n",
    "    (\"property_type\", FloatType),\n",
    "    (\"occupancy_status\", FloatType),\n",
    "    (\"property_state\", FloatType),\n",
    "    (\"product_type\", FloatType),\n",
    "    (\"relocation_mortgage_indicator\", FloatType),\n",
    "    (\"seller_name\", FloatType),\n",
    "    (\"mod_flag\", FloatType)\n",
    "  )\n",
    "\n",
    "val numericCols = List(\n",
    "    (\"orig_interest_rate\", FloatType),\n",
    "    (\"orig_upb\", IntegerType),\n",
    "    (\"orig_loan_term\", IntegerType),\n",
    "    (\"orig_ltv\", FloatType),\n",
    "    (\"orig_cltv\", FloatType),\n",
    "    (\"num_borrowers\", FloatType),\n",
    "    (\"dti\", FloatType),\n",
    "    (\"borrower_credit_score\", FloatType),\n",
    "    (\"num_units\", IntegerType),\n",
    "    (\"zip\", IntegerType),\n",
    "    (\"mortgage_insurance_percent\", FloatType),\n",
    "    (\"current_loan_delinquency_status\", IntegerType),\n",
    "    (\"current_actual_upb\", FloatType),\n",
    "    (\"interest_rate\", FloatType),\n",
    "    (\"loan_age\", FloatType),\n",
    "    (\"msa\", FloatType),\n",
    "    (\"non_interest_bearing_upb\", FloatType),\n",
    "    (labelColName, IntegerType)\n",
    "  )\n",
    "\n",
    "val commParamMap = Map(\n",
    "    \"eta\" -> 0.1,\n",
    "    \"gamma\" -> 0.1,\n",
    "    \"missing\" -> 0.0,\n",
    "    \"max_depth\" -> 10,\n",
    "    \"max_leaves\" -> 256,\n",
    "    \"objective\" -> \"binary:logistic\",\n",
    "    \"grow_policy\" -> \"depthwise\",\n",
    "    \"min_child_weight\" -> 30,\n",
    "    \"lambda\" -> 1,\n",
    "    \"scale_pos_weight\" -> 2,\n",
    "    \"subsample\" -> 1,\n",
    "    \"nthread\" -> 1,\n",
    "    \"num_round\" -> 100)\n",
    "var cachedDictDF: DataFrame = _"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177b6b8",
   "metadata": {},
   "source": [
    "* Define Casting Process\n",
    "This part is casting String column to Numbric. \n",
    "Example:\n",
    "```\n",
    "col_1\n",
    " \"a\"\n",
    " \"b\"\n",
    " \"c\"\n",
    " \"a\"\n",
    "# After String ====> Numberic\n",
    "col_1\n",
    " 0\n",
    " 1\n",
    " 2\n",
    " 0\n",
    "```  \n",
    "<br>\n",
    "\n",
    "* Define function to get column dictionary\n",
    "\n",
    "    Example\n",
    "    ```\n",
    "    col1 = [row(data=\"a\",id=0), row(data=\"b\",id=1)]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5091c8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genDictionary: (etlDF: org.apache.spark.sql.DataFrame, colNames: Seq[String])org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def genDictionary(etlDF: DataFrame, colNames: Seq[String]): DataFrame = {\n",
    "    val cntTable = etlDF\n",
    "      .select(posexplode(array(colNames.map(col(_)): _*)))\n",
    "      .withColumnRenamed(\"pos\", \"column_id\")\n",
    "      .withColumnRenamed(\"col\", \"data\")\n",
    "      .filter(\"data is not null\")\n",
    "      .groupBy(\"column_id\", \"data\")\n",
    "      .count()\n",
    "    val windowed = Window.partitionBy(\"column_id\").orderBy(desc(\"count\"))\n",
    "    cntTable\n",
    "      .withColumn(\"id\", row_number().over(windowed))\n",
    "      .drop(\"count\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466af65",
   "metadata": {},
   "source": [
    "* Define function to convert string columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9df8fe60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "castStringColumnsToNumeric: (inputDF: org.apache.spark.sql.DataFrame, spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def castStringColumnsToNumeric(inputDF: DataFrame, spark: SparkSession): DataFrame = {\n",
    "    val cateColNames = categaryCols.map(_._1)\n",
    "    cachedDictDF = genDictionary(inputDF, cateColNames).cache()\n",
    "\n",
    "    // Generate the final table with all columns being numeric.\n",
    "    cateColNames.foldLeft(inputDF) {\n",
    "      case (df, colName) =>\n",
    "        val colPos = cateColNames.indexOf(colName)\n",
    "        val colDictDF = cachedDictDF\n",
    "          .filter(col(\"column_id\") === colPos)\n",
    "          .drop(\"column_id\")\n",
    "          .withColumnRenamed(\"data\", colName)\n",
    "        df.join(broadcast(colDictDF), Seq(colName), \"left\")\n",
    "          .drop(colName)\n",
    "          .withColumnRenamed(\"id\", colName)\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c64d85",
   "metadata": {},
   "source": [
    "* Build the spark session and data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98d37174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2777350c\n",
       "reader: org.apache.spark.sql.DataFrameReader = org.apache.spark.sql.DataFrameReader@1545dee\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Build the spark session and data reader as usual\n",
    "val sparkSession = SparkSession.builder.appName(\"mortgage-gpu\").getOrCreate\n",
    "val reader = spark.read.option(\"header\", false)\n",
    "                              .option(\"nullValue\", \"\")\n",
    "                              .option(\"delimiter\", \"|\")\n",
    "                              .option(\"parserLib\", \"univocity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b5456",
   "metadata": {},
   "source": [
    "* Read CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "smoking-coordination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawSet: org.apache.spark.sql.DataFrame = [reference_pool_id: string, loan_id: bigint ... 106 more fields]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rawSet = reader\n",
    "      .schema(rawSchema)\n",
    "      .csv(rawPath)\n",
    "rawSet.createOrReplaceTempView(\"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "imperial-specific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perfSet: org.apache.spark.sql.DataFrame = [loan_id: bigint, monthly_reporting_period: string ... 30 more fields]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val perfSet = spark.sql(\"\"\"\n",
    "select loan_id, \n",
    "       date_format(to_date(monthly_reporting_period,\"MMyyyy\"), \"MM/dd/yyyy\") as monthly_reporting_period,\n",
    "       upper(servicer) as servicer,\n",
    "       interest_rate,\n",
    "       current_actual_upb,\n",
    "       loan_age,\n",
    "       remaining_months_to_legal_maturity,\n",
    "       adj_remaining_months_to_maturity,\n",
    "       date_format(to_date(maturity_date,\"MMyyyy\"), \"MM/yyyy\") as maturity_date,\n",
    "       msa,\n",
    "       current_loan_delinquency_status,\n",
    "       mod_flag,\n",
    "       zero_balance_code,\n",
    "       date_format(to_date(zero_balance_effective_date,\"MMyyyy\"), \"MM/yyyy\") as zero_balance_effective_date,\n",
    "       date_format(to_date(last_paid_installment_date,\"MMyyyy\"), \"MM/dd/yyyy\") as last_paid_installment_date,\n",
    "       date_format(to_date(foreclosed_after,\"MMyyyy\"), \"MM/dd/yyyy\") as foreclosed_after,\n",
    "       date_format(to_date(disposition_date,\"MMyyyy\"), \"MM/dd/yyyy\") as disposition_date,\n",
    "       foreclosure_costs,\n",
    "       prop_preservation_and_repair_costs,\n",
    "       asset_recovery_costs,\n",
    "       misc_holding_expenses,\n",
    "       holding_taxes,\n",
    "       net_sale_proceeds,\n",
    "       credit_enhancement_proceeds,\n",
    "       repurchase_make_whole_proceeds,\n",
    "       other_foreclosure_proceeds,\n",
    "       non_interest_bearing_upb,\n",
    "       principal_forgiveness_upb,\n",
    "       repurchase_make_whole_proceeds_flag,\n",
    "       foreclosure_principal_write_off_amount,\n",
    "       servicing_activity_indicator,\n",
    "       substring_index(substring_index(input_file_name(),'/',-1),'.',1) as quarter\n",
    "from raw\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bac2301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acqSet: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [loan_id: bigint, orig_channel: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val acqSet = spark.sql(\"\"\"\n",
    "select loan_id,\n",
    "       orig_channel,\n",
    "       seller_name,\n",
    "       orig_interest_rate,\n",
    "       orig_upb,\n",
    "       orig_loan_term,\n",
    "       date_format(to_date(orig_date,\"MMyyyy\"), \"MM/yyyy\") as orig_date,\n",
    "       date_format(to_date(first_pay_date,\"MMyyyy\"), \"MM/yyyy\") as first_pay_date,\n",
    "       orig_ltv,\n",
    "       orig_cltv,\n",
    "       num_borrowers,\n",
    "       dti,\n",
    "       borrower_credit_score,\n",
    "       first_home_buyer,\n",
    "       loan_purpose,\n",
    "       property_type,\n",
    "       num_units,\n",
    "       occupancy_status,\n",
    "       property_state,\n",
    "       zip,\n",
    "       mortgage_insurance_percent,\n",
    "       product_type,\n",
    "       coborrow_credit_score,\n",
    "       mortgage_insurance_type,\n",
    "       relocation_mortgage_indicator,\n",
    "       substring_index(substring_index(input_file_name(),'/',-1),'.',1) as quarter\n",
    "from raw\n",
    "\"\"\").distinct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c814c8",
   "metadata": {},
   "source": [
    "* Define ETL Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a16155cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined trait MortgageETL\n",
       "allCols: List[org.apache.spark.sql.Column] = List(orig_channel, first_home_buyer, loan_purpose, property_type, occupancy_status, property_state, product_type, relocation_mortgage_indicator, seller_name, mod_flag, orig_interest_rate, orig_upb, orig_loan_term, orig_ltv, orig_cltv, num_borrowers, dti, borrower_credit_score, num_units, zip, mortgage_insurance_percent, current_loan_delinquency_status, current_actual_upb, interest_rate, loan_age, msa, non_interest_bearing_upb, delinquency_12)\n",
       "defined object PerformanceETL\n",
       "defined object AcquisitionETL\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trait MortgageETL {\n",
    "  var dataFrame: DataFrame = _\n",
    "\n",
    "  def from(df: DataFrame): this.type = {\n",
    "    dataFrame = df\n",
    "    this\n",
    "  }\n",
    "}\n",
    "val allCols = (categaryCols ++ numericCols).map(c => col(c._1))\n",
    "\n",
    "object PerformanceETL extends MortgageETL {\n",
    "\n",
    "  def prepare: this.type = {\n",
    "    dataFrame = dataFrame\n",
    "      .withColumn(\"monthly_reporting_period\", to_date(col(\"monthly_reporting_period\"), \"MM/dd/yyyy\"))\n",
    "      .withColumn(\"monthly_reporting_period_month\", month(col(\"monthly_reporting_period\")))\n",
    "      .withColumn(\"monthly_reporting_period_year\", year(col(\"monthly_reporting_period\")))\n",
    "      .withColumn(\"monthly_reporting_period_day\", dayofmonth(col(\"monthly_reporting_period\")))\n",
    "      .withColumn(\"last_paid_installment_date\", to_date(col(\"last_paid_installment_date\"), \"MM/dd/yyyy\"))\n",
    "      .withColumn(\"foreclosed_after\", to_date(col(\"foreclosed_after\"), \"MM/dd/yyyy\"))\n",
    "      .withColumn(\"disposition_date\", to_date(col(\"disposition_date\"), \"MM/dd/yyyy\"))\n",
    "      .withColumn(\"maturity_date\", to_date(col(\"maturity_date\"), \"MM/yyyy\"))\n",
    "      .withColumn(\"zero_balance_effective_date\", to_date(col(\"zero_balance_effective_date\"), \"MM/yyyy\"))\n",
    "      .withColumn(\"current_actual_upb\", col(\"current_actual_upb\"))\n",
    "      .withColumn(\"current_loan_delinquency_status\", col(\"current_loan_delinquency_status\"))\n",
    "    this\n",
    "  }\n",
    "\n",
    "  def createDelinquency(spark: SparkSession): this.type = {\n",
    "    val aggDF = dataFrame\n",
    "      .select(\n",
    "        col(\"quarter\"),\n",
    "        col(\"loan_id\"),\n",
    "        col(\"current_loan_delinquency_status\"),\n",
    "        when(col(\"current_loan_delinquency_status\") >= 1, col(\"monthly_reporting_period\")).alias(\"delinquency_30\"),\n",
    "        when(col(\"current_loan_delinquency_status\") >= 3, col(\"monthly_reporting_period\")).alias(\"delinquency_90\"),\n",
    "        when(col(\"current_loan_delinquency_status\") >= 6, col(\"monthly_reporting_period\")).alias(\"delinquency_180\")\n",
    "      )\n",
    "      .groupBy(\"quarter\", \"loan_id\")\n",
    "      .agg(\n",
    "        max(\"current_loan_delinquency_status\").alias(\"delinquency_12\"),\n",
    "        min(\"delinquency_30\").alias(\"delinquency_30\"),\n",
    "        min(\"delinquency_90\").alias(\"delinquency_90\"),\n",
    "        min(\"delinquency_180\").alias(\"delinquency_180\")\n",
    "      )\n",
    "      .select(\n",
    "        col(\"quarter\"),\n",
    "        col(\"loan_id\"),\n",
    "        (col(\"delinquency_12\") >= 1).alias(\"ever_30\"),\n",
    "        (col(\"delinquency_12\") >= 3).alias(\"ever_90\"),\n",
    "        (col(\"delinquency_12\") >= 6).alias(\"ever_180\"),\n",
    "        col(\"delinquency_30\"),\n",
    "        col(\"delinquency_90\"),\n",
    "        col(\"delinquency_180\")\n",
    "      )\n",
    "\n",
    "    val joinedDf = dataFrame\n",
    "      .withColumnRenamed(\"monthly_reporting_period\", \"timestamp\")\n",
    "      .withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\")\n",
    "      .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\")\n",
    "      .withColumnRenamed(\"current_loan_delinquency_status\", \"delinquency_12\")\n",
    "      .withColumnRenamed(\"current_actual_upb\", \"upb_12\")\n",
    "      .select(\"quarter\", \"loan_id\", \"timestamp\", \"delinquency_12\", \"upb_12\", \"timestamp_month\", \"timestamp_year\")\n",
    "      .join(aggDF, Seq(\"loan_id\", \"quarter\"), \"left_outer\")\n",
    "\n",
    "    // calculate the 12 month delinquency and upb values\n",
    "    val months = 12\n",
    "    val monthArray = 0.until(months).toArray\n",
    "    val testDf = joinedDf\n",
    "      // explode on a small amount of data is actually slightly more efficient than a cross join\n",
    "      .withColumn(\"month_y\", explode(lit(monthArray)))\n",
    "      .select(\n",
    "        col(\"quarter\"),\n",
    "        floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000) / months).alias(\"josh_mody\"),\n",
    "        floor(((col(\"timestamp_year\") * 12 + col(\"timestamp_month\")) - 24000 - col(\"month_y\")) / months).alias(\"josh_mody_n\"),\n",
    "        col(\"ever_30\"),\n",
    "        col(\"ever_90\"),\n",
    "        col(\"ever_180\"),\n",
    "        col(\"delinquency_30\"),\n",
    "        col(\"delinquency_90\"),\n",
    "        col(\"delinquency_180\"),\n",
    "        col(\"loan_id\"),\n",
    "        col(\"month_y\"),\n",
    "        col(\"delinquency_12\"),\n",
    "        col(\"upb_12\")\n",
    "      )\n",
    "      .groupBy(\"quarter\", \"loan_id\", \"josh_mody_n\", \"ever_30\", \"ever_90\", \"ever_180\", \"delinquency_30\", \"delinquency_90\", \"delinquency_180\", \"month_y\")\n",
    "      .agg(max(\"delinquency_12\").alias(\"delinquency_12\"), min(\"upb_12\").alias(\"upb_12\"))\n",
    "      .withColumn(\"timestamp_year\", floor((lit(24000) + (col(\"josh_mody_n\") * lit(months)) + (col(\"month_y\") - 1)) / lit(12)))\n",
    "      .withColumn(\"timestamp_month_tmp\", pmod(lit(24000) + (col(\"josh_mody_n\") * lit(months)) + col(\"month_y\"), lit(12)))\n",
    "      .withColumn(\"timestamp_month\", when(col(\"timestamp_month_tmp\") === lit(0), lit(12)).otherwise(col(\"timestamp_month_tmp\")))\n",
    "      .withColumn(\"delinquency_12\", ((col(\"delinquency_12\") > 3).cast(\"int\") + (col(\"upb_12\") === 0).cast(\"int\")).alias(\"delinquency_12\"))\n",
    "      .drop(\"timestamp_month_tmp\", \"josh_mody_n\", \"month_y\")\n",
    "\n",
    "    dataFrame = dataFrame\n",
    "      .withColumnRenamed(\"monthly_reporting_period_month\", \"timestamp_month\")\n",
    "      .withColumnRenamed(\"monthly_reporting_period_year\", \"timestamp_year\")\n",
    "      .join(testDf, Seq(\"quarter\", \"loan_id\", \"timestamp_year\", \"timestamp_month\"), \"left\").drop(\"timestamp_year\", \"timestamp_month\")\n",
    "    this\n",
    "  }\n",
    "}\n",
    "\n",
    "object AcquisitionETL extends MortgageETL {\n",
    "\n",
    "  def createAcquisition(spark: SparkSession): this.type = {\n",
    "    val nameMapping = NameMapping(spark, \"from_seller_name\", \"to_seller_name\")\n",
    "    dataFrame = dataFrame\n",
    "      .join(nameMapping, col(\"seller_name\") === col(\"from_seller_name\"), \"left\")\n",
    "      .drop(\"from_seller_name\")\n",
    "      /* backup the original name before we replace it */\n",
    "      .withColumn(\"old_name\", col(\"seller_name\"))\n",
    "      /* replace seller_name with the new version if we found one in the mapping, or the old version\n",
    "       if we didn't */\n",
    "      .withColumn(\"seller_name\", coalesce(col(\"to_seller_name\"), col(\"seller_name\")))\n",
    "      .drop(\"to_seller_name\")\n",
    "      .withColumn(\"orig_date\", to_date(col(\"orig_date\"), \"MM/yyyy\"))\n",
    "      .withColumn(\"first_pay_date\", to_date(col(\"first_pay_date\"), \"MM/yyyy\"))\n",
    "    this\n",
    "  }\n",
    "\n",
    "  def cleanPrime(perfDF: DataFrame): this.type = {\n",
    "    dataFrame = perfDF.join(dataFrame, Seq(\"loan_id\", \"quarter\"), \"inner\").drop(\"quarter\")\n",
    "    this\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78b76252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transform: (perfDF: org.apache.spark.sql.DataFrame, acqDF: org.apache.spark.sql.DataFrame, spark: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(perfDF: DataFrame, acqDF: DataFrame, spark: SparkSession): DataFrame = {\n",
    "    val etlPerfDF = PerformanceETL.from(perfDF)\n",
    "      .prepare\n",
    "      .createDelinquency(spark)\n",
    "      .dataFrame\n",
    "    val cleanDF = AcquisitionETL.from(acqDF)\n",
    "      .createAcquisition(spark)\n",
    "      .cleanPrime(etlPerfDF)\n",
    "      .dataFrame\n",
    "\n",
    "    // Convert to xgb required Dataset\n",
    "    castStringColumnsToNumeric(cleanDF, spark)\n",
    "      .select(allCols: _*)\n",
    "      .withColumn(labelColName, when(col(labelColName) > 0, 1).otherwise(0))\n",
    "      .na.fill(0.0f)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1234f49",
   "metadata": {},
   "source": [
    "## Run ETL Process and Save the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffdb0a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.",
      "  at org.apache.spark.sql.rapids.GpuFileFormatWriter$.write(GpuFileFormatWriter.scala:263)",
      "  at org.apache.spark.sql.rapids.GpuInsertIntoHadoopFsRelationCommand.runColumnar(GpuInsertIntoHadoopFsRelationCommand.scala:166)",
      "  at com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult$lzycompute(GpuDataWritingCommandExec.scala:97)",
      "  at com.nvidia.spark.rapids.GpuDataWritingCommandExec.sideEffectResult(GpuDataWritingCommandExec.scala:96)",
      "  at com.nvidia.spark.rapids.GpuDataWritingCommandExec.doExecuteColumnar(GpuDataWritingCommandExec.scala:120)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:211)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:207)",
      "  at com.nvidia.spark.rapids.GpuColumnarToRowExec.doExecute(GpuColumnarToRowExec.scala:320)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)",
      "  at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:325)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:391)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)",
      "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:781)",
      "  ... 41 elided",
      "Caused by: java.lang.reflect.InvocationTargetException: org.apache.spark.sql.catalyst.parser.ParseException:",
      "mismatched input '<EOF>' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 0)",
      "",
      "== SQL ==",
      "",
      "^^^",
      "",
      "  at sun.reflect.GeneratedConstructorAccessor33.newInstance(Unknown Source)",
      "  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
      "  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$makeCopy$6(TreeNode.scala:738)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:737)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:694)",
      "  at org.apache.spark.sql.execution.SparkPlan.super$makeCopy(SparkPlan.scala:88)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$makeCopy$1(SparkPlan.scala:88)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SparkPlan.makeCopy(SparkPlan.scala:88)",
      "  at org.apache.spark.sql.execution.SparkPlan.makeCopy(SparkPlan.scala:59)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.legacyWithNewChildren(TreeNode.scala:413)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.com$nvidia$spark$rapids$shims$ShimUnaryExecNode$$super$legacyWithNewChildren(basicPhysicalOperators.scala:313)",
      "  at com.nvidia.spark.rapids.shims.ShimUnaryExecNode.withNewChildInternal(TreeNode.scala:57)",
      "  at com.nvidia.spark.rapids.shims.ShimUnaryExecNode.withNewChildInternal$(TreeNode.scala:56)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.withNewChildInternal(basicPhysicalOperators.scala:313)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.withNewChildInternal(basicPhysicalOperators.scala:313)",
      "  at org.apache.spark.sql.catalyst.trees.UnaryLike.withNewChildrenInternal(TreeNode.scala:1136)",
      "  at org.apache.spark.sql.catalyst.trees.UnaryLike.withNewChildrenInternal$(TreeNode.scala:1134)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.withNewChildrenInternal(basicPhysicalOperators.scala:313)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$withNewChildren$2(TreeNode.scala:359)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:358)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize(GpuExec.scala:334)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize$(GpuExec.scala:310)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.doCanonicalize(basicPhysicalOperators.scala:313)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.doCanonicalize(basicPhysicalOperators.scala:313)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:485)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:484)",
      "  at com.nvidia.spark.rapids.GpuExec.$anonfun$doCanonicalize$1(GpuExec.scala:311)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize(GpuExec.scala:311)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize$(GpuExec.scala:310)",
      "  at com.nvidia.spark.rapids.GpuCoalesceBatches.doCanonicalize(GpuCoalesceBatches.scala:580)",
      "  at com.nvidia.spark.rapids.GpuCoalesceBatches.doCanonicalize(GpuCoalesceBatches.scala:580)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:485)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:484)",
      "  at com.nvidia.spark.rapids.GpuExec.$anonfun$doCanonicalize$1(GpuExec.scala:311)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize(GpuExec.scala:311)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize$(GpuExec.scala:310)",
      "  at com.nvidia.spark.rapids.GpuProjectExec.doCanonicalize(basicPhysicalOperators.scala:115)",
      "  at com.nvidia.spark.rapids.GpuProjectExec.doCanonicalize(basicPhysicalOperators.scala:115)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:485)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:484)",
      "  at com.nvidia.spark.rapids.GpuExec.$anonfun$doCanonicalize$1(GpuExec.scala:311)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize(GpuExec.scala:311)",
      "  at com.nvidia.spark.rapids.GpuExec.doCanonicalize$(GpuExec.scala:310)",
      "  at org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase.doCanonicalize(GpuShuffleExchangeExecBase.scala:147)",
      "  at org.apache.spark.sql.rapids.execution.GpuShuffleExchangeExecBase.doCanonicalize(GpuShuffleExchangeExecBase.scala:147)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:485)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:484)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:493)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:483)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$createQueryStages$2(AdaptiveSparkPlanExec.scala:517)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)",
      "  at scala.collection.Iterator.foreach(Iterator.scala:943)",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:943)",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)",
      "  at scala.collection.IterableLike.foreach(IterableLike.scala:74)",
      "  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)",
      "  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.createQueryStages(AdaptiveSparkPlanExec.scala:517)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:231)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:226)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:365)",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecuteColumnar(AdaptiveSparkPlanExec.scala:354)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:211)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:207)",
      "  at org.apache.spark.sql.rapids.GpuFileFormatWriter$.write(GpuFileFormatWriter.scala:199)",
      "  ... 84 more",
      "Caused by: org.apache.spark.sql.catalyst.parser.ParseException:",
      "mismatched input '<EOF>' expecting {'ADD', 'AFTER', 'ALL', 'ALTER', 'ANALYZE', 'AND', 'ANTI', 'ANY', 'ARCHIVE', 'ARRAY', 'AS', 'ASC', 'AT', 'AUTHORIZATION', 'BETWEEN', 'BOTH', 'BUCKET', 'BUCKETS', 'BY', 'CACHE', 'CASCADE', 'CASE', 'CAST', 'CHANGE', 'CHECK', 'CLEAR', 'CLUSTER', 'CLUSTERED', 'CODEGEN', 'COLLATE', 'COLLECTION', 'COLUMN', 'COLUMNS', 'COMMENT', 'COMMIT', 'COMPACT', 'COMPACTIONS', 'COMPUTE', 'CONCATENATE', 'CONSTRAINT', 'COST', 'CREATE', 'CROSS', 'CUBE', 'CURRENT', 'CURRENT_DATE', 'CURRENT_TIME', 'CURRENT_TIMESTAMP', 'CURRENT_USER', 'DAY', 'DATA', 'DATABASE', DATABASES, 'DBPROPERTIES', 'DEFINED', 'DELETE', 'DELIMITED', 'DESC', 'DESCRIBE', 'DFS', 'DIRECTORIES', 'DIRECTORY', 'DISTINCT', 'DISTRIBUTE', 'DIV', 'DROP', 'ELSE', 'END', 'ESCAPE', 'ESCAPED', 'EXCEPT', 'EXCHANGE', 'EXISTS', 'EXPLAIN', 'EXPORT', 'EXTENDED', 'EXTERNAL', 'EXTRACT', 'FALSE', 'FETCH', 'FIELDS', 'FILTER', 'FILEFORMAT', 'FIRST', 'FOLLOWING', 'FOR', 'FOREIGN', 'FORMAT', 'FORMATTED', 'FROM', 'FULL', 'FUNCTION', 'FUNCTIONS', 'GLOBAL', 'GRANT', 'GROUP', 'GROUPING', 'HAVING', 'HOUR', 'IF', 'IGNORE', 'IMPORT', 'IN', 'INDEX', 'INDEXES', 'INNER', 'INPATH', 'INPUTFORMAT', 'INSERT', 'INTERSECT', 'INTERVAL', 'INTO', 'IS', 'ITEMS', 'JOIN', 'KEYS', 'LAST', 'LATERAL', 'LAZY', 'LEADING', 'LEFT', 'LIKE', 'LIMIT', 'LINES', 'LIST', 'LOAD', 'LOCAL', 'LOCATION', 'LOCK', 'LOCKS', 'LOGICAL', 'MACRO', 'MAP', 'MATCHED', 'MERGE', 'MINUTE', 'MONTH', 'MSCK', 'NAMESPACE', 'NAMESPACES', 'NATURAL', 'NO', NOT, 'NULL', 'NULLS', 'OF', 'ON', 'ONLY', 'OPTION', 'OPTIONS', 'OR', 'ORDER', 'OUT', 'OUTER', 'OUTPUTFORMAT', 'OVER', 'OVERLAPS', 'OVERLAY', 'OVERWRITE', 'PARTITION', 'PARTITIONED', 'PARTITIONS', 'PERCENT', 'PIVOT', 'PLACING', 'POSITION', 'PRECEDING', 'PRIMARY', 'PRINCIPALS', 'PROPERTIES', 'PURGE', 'QUERY', 'RANGE', 'RECORDREADER', 'RECORDWRITER', 'RECOVER', 'REDUCE', 'REFERENCES', 'REFRESH', 'RENAME', 'REPAIR', 'REPLACE', 'RESET', 'RESPECT', 'RESTRICT', 'REVOKE', 'RIGHT', RLIKE, 'ROLE', 'ROLES', 'ROLLBACK', 'ROLLUP', 'ROW', 'ROWS', 'SECOND', 'SCHEMA', 'SELECT', 'SEMI', 'SEPARATED', 'SERDE', 'SERDEPROPERTIES', 'SESSION_USER', 'SET', 'MINUS', 'SETS', 'SHOW', 'SKEWED', 'SOME', 'SORT', 'SORTED', 'START', 'STATISTICS', 'STORED', 'STRATIFY', 'STRUCT', 'SUBSTR', 'SUBSTRING', 'SYNC', 'TABLE', 'TABLES', 'TABLESAMPLE', 'TBLPROPERTIES', TEMPORARY, 'TERMINATED', 'THEN', 'TIME', 'TO', 'TOUCH', 'TRAILING', 'TRANSACTION', 'TRANSACTIONS', 'TRANSFORM', 'TRIM', 'TRUE', 'TRUNCATE', 'TRY_CAST', 'TYPE', 'UNARCHIVE', 'UNBOUNDED', 'UNCACHE', 'UNION', 'UNIQUE', 'UNKNOWN', 'UNLOCK', 'UNSET', 'UPDATE', 'USE', 'USER', 'USING', 'VALUES', 'VIEW', 'VIEWS', 'WHEN', 'WHERE', 'WINDOW', 'WITH', 'YEAR', 'ZONE', IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 0)",
      "",
      "== SQL ==",
      "",
      "^^^",
      "",
      "  at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:266)",
      "  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:127)",
      "  at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseMultipartIdentifier(ParseDriver.scala:63)",
      "  at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute$.apply(unresolved.scala:184)",
      "  at com.nvidia.spark.rapids.GpuAlias.toAttribute(namedExpressions.scala:61)",
      "  at com.nvidia.spark.rapids.GpuProjectExec.$anonfun$output$1(basicPhysicalOperators.scala:131)",
      "  at scala.collection.immutable.List.map(List.scala:297)",
      "  at com.nvidia.spark.rapids.GpuProjectExec.output(basicPhysicalOperators.scala:131)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.outputSet$lzycompute(QueryPlan.scala:54)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.outputSet(QueryPlan.scala:54)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.$anonfun$notNullPreds$1(basicPhysicalOperators.scala:324)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.$anonfun$notNullPreds$1$adapted(basicPhysicalOperators.scala:323)",
      "  at scala.collection.TraversableLike.$anonfun$partition$1(TraversableLike.scala:450)",
      "  at scala.collection.immutable.List.foreach(List.scala:431)",
      "  at scala.collection.TraversableLike.partition(TraversableLike.scala:450)",
      "  at scala.collection.TraversableLike.partition$(TraversableLike.scala:448)",
      "  at scala.collection.AbstractTraversable.partition(Traversable.scala:108)",
      "  at com.nvidia.spark.rapids.GpuFilterExec.<init>(basicPhysicalOperators.scala:323)",
      "  ... 490 more",
      ""
     ]
    }
   ],
   "source": [
    "val t0 = System.currentTimeMillis\n",
    "val rawDF = transform(\n",
    "      perfSet,\n",
    "      acqSet,\n",
    "      sparkSession\n",
    "    )\n",
    "rawDF.write.mode(\"overwrite\").parquet(new Path(outPath, \"data\").toString)\n",
    "val t1 = System.currentTimeMillis\n",
    "println(\"Elapsed time : \" + ((t1 - t0).toFloat / 1000) + \"s\")\n",
    "sparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4388fe96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
